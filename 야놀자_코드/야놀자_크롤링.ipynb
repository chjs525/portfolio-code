{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf9458b",
   "metadata": {},
   "source": [
    "# ì•¼ë†€ì ì²´í¬ì¸, ì²´í¬ì•„ì›ƒ ëˆ„ë½ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f180b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(0.4)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°\n",
    "# search_input = driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input')\n",
    "# time.sleep(0.8)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°\n",
    "\n",
    "# search_input.send_keys('ê´´ì‚° ë”ì§„íœì…˜')\n",
    "# search_input.send_keys(Keys.ENTER)\n",
    "# time.sleep(1.2)\n",
    "# driver.find_element(By.XPATH , value = '/html/body/div[1]/div/div[2]/div/div[1]/a/div/div[2]/div[1]/p').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ì²´í¬ = pd.read_csv('0807ì²´í¬ì¸ì•„ì›ƒëˆ„ë½v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(0.4)\n",
    "all_product_CD = all_product_CD['LDGS_CD'].tolist()\n",
    "\n",
    "def ì•¼ë†€ì(all_product_CD):\n",
    "    data = []\n",
    "\n",
    "    for idx, pid in enumerate(all_product_CD):\n",
    "        url = f'https://place-site.yanolja.com/places/{pid}'\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "        prev_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            # ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1.5)  # ë¡œë”© ëŒ€ê¸°\n",
    "\n",
    "            # ìŠ¤í¬ë¡¤ í›„ ë†’ì´ ì¸¡ì •\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            # ë” ì´ìƒ ìŠ¤í¬ë¡¤í•  ê³³ ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
    "            if new_height == prev_height:\n",
    "                break\n",
    "            prev_height = new_height\n",
    "\n",
    "\n",
    "\n",
    "            time.sleep(1.2)\n",
    "            driver.find_element(By.XPATH , value = '//*[@id=\"domestic-pdp-info\"]/div[5]/div/div/div[3]/div/button').click()\n",
    "\n",
    "\n",
    "\n",
    "            # ì²´í¬ì¸\n",
    "            CHIN_TIME = soup.find('div', class_='css-1yzvfwv')\n",
    "            CHIN_TIME = CHIN_TIME.text if CHIN_TIME else 'ì •ë³´ ì—†ìŒ'\n",
    "\n",
    "\n",
    "            # ì²´í¬ì•„ì›ƒ\n",
    "            CHOT_TIME = soup.find('span', class_='css-1yzvfwv')\n",
    "            CHOT_TIME = CHOT_TIME.text if CHOT_TIME else 'ì •ë³´ ì—†ìŒ'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • (ë´‡ ê°ì§€ ìš°íšŒ) ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì˜¤í”ˆ (ì¿ í‚¤ ì„¤ì • ë“± ì´ˆê¸°í™” ëª©ì ) ===\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìƒí’ˆ ID ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "all_product_CD = pd.read_csv('ì²´í¬ì¸ì•„ì›ƒëˆ„ë½.csv')\n",
    "all_product_CD['LDGS_CD'] = all_product_CD['LDGS_CD'].astype(str)\n",
    "all_product_CD = all_product_CD['LDGS_CD'].tolist()\n",
    "\n",
    "# === 4. í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ ===\n",
    "def ì•¼ë†€ì(driver, all_product_CD):\n",
    "    data = []\n",
    "\n",
    "    for idx, pid in enumerate(all_product_CD):\n",
    "        print(f\"[{idx+1}] ìˆ™ì†Œ ID {pid} í˜ì´ì§€ ì ‘ì† ì¤‘...\")\n",
    "        place_url = f'https://place-site.yanolja.com/places/{pid}'\n",
    "        driver.get(place_url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # === ìŠ¤í¬ë¡¤ ë‹¤ìš´ ===\n",
    "        try:\n",
    "            prev_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            while True:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.5)\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == prev_height:\n",
    "                    break\n",
    "                prev_height = new_height\n",
    "        except:\n",
    "            print(\"ìŠ¤í¬ë¡¤ ì‹¤íŒ¨\")\n",
    "\n",
    "        # === ë²„íŠ¼ í´ë¦­ (í•„ìš” ì‹œ) ===\n",
    "        try:\n",
    "            btn = driver.find_element(By.XPATH, '//*[@id=\"domestic-pdp-info\"]/div[5]/div/div/div[3]/div/button')\n",
    "            btn.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            pass  # ë²„íŠ¼ ì—†ì„ ìˆ˜ë„ ìˆìŒ\n",
    "\n",
    "        # === ë°ì´í„° ì¶”ì¶œ ===\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        checkin = soup.find('div', class_='css-1yzvfwv')\n",
    "        checkout = soup.find('span', class_='css-1yzvfwv')\n",
    "\n",
    "        checkin = checkin.text.strip() if checkin else 'ì •ë³´ ì—†ìŒ'\n",
    "        checkout = checkout.text.strip() if checkout else 'ì •ë³´ ì—†ìŒ'\n",
    "\n",
    "        data.append({\n",
    "            'LDGS_CD': pid,\n",
    "            'ì²´í¬ì¸': checkin,\n",
    "            'ì²´í¬ì•„ì›ƒ': checkout\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# === 5. ì‹¤í–‰ ë° ì¶œë ¥ ===\n",
    "result = ì•¼ë†€ì(driver, all_product_CD)\n",
    "\n",
    "# ì½˜ì†” ì¶œë ¥\n",
    "for item in result:\n",
    "    print(item)\n",
    "\n",
    "# CSV ì €ì¥\n",
    "df = pd.DataFrame(result)\n",
    "# df.to_csv(\"ì•¼ë†€ì_ì²´í¬ì¸_ì²´í¬ì•„ì›ƒ.csv\", index=False, encoding='utf-8-sig')\n",
    "# print(\"CSV ì €ì¥ ì™„ë£Œ.\")\n",
    "df\n",
    "# ë“œë¼ì´ë²„ ì¢…ë£Œ (í•„ìš” ì‹œ)\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ì²´í¬ = pd.read_csv('0807ì²´í¬ì¸ì•„ì›ƒëˆ„ë½v1.csv')\n",
    "df_ì²´í¬['LDGMNT_TY_NM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDGMNT_TY_NM Pensionë§Œ ì¶”ì¶œ\n",
    "pension_only = all_product_CD[all_product_CD['LDGMNT_TY_NM'] == 'Hotel']\n",
    "pension_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631a1a8",
   "metadata": {},
   "source": [
    "## íœì…˜ ì²´í¬ì¸ ì²´í¬ì•„ì›ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45575f",
   "metadata": {},
   "source": [
    "- 10ê°œë§Œ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ë° ìƒìœ„ 10ê°œ ì¶”ì¶œ ===\n",
    "df = pd.read_csv('ì²´í¬ì¸ì•„ì›ƒëˆ„ë½.csv')\n",
    "df = df[df['LDGMNT_TY_NM'] == 'Pension']\n",
    "ids = df['LDGS_CD'].astype(str).tolist()[:10]\n",
    "\n",
    "# === 4. í¬ë¡¤ë§ í•¨ìˆ˜ ===\n",
    "def ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, id_list):\n",
    "    result = []\n",
    "\n",
    "    for idx, pid in enumerate(id_list):\n",
    "        print(f\"\\n[{idx+1}] ìˆ™ì†Œ ID {pid} ì ‘ì† ì¤‘...\")\n",
    "        driver.get(f\"https://place-site.yanolja.com/places/{pid}\")\n",
    "        time.sleep(2.5)\n",
    "\n",
    "        # === ì „ì²´ë³´ê¸° ë²„íŠ¼ í´ë¦­ ===\n",
    "        try:\n",
    "            btn = driver.find_element(By.CSS_SELECTOR, 'button.css-ff1hhr')\n",
    "            btn.click()\n",
    "            print(\"âœ… ì „ì²´ë³´ê¸° í´ë¦­ ì„±ê³µ\")\n",
    "            time.sleep(1.5)\n",
    "        except:\n",
    "            print(\"âŒ ì „ì²´ë³´ê¸° ë²„íŠ¼ ì—†ìŒ\")\n",
    "\n",
    "        # === iframe ë‚´ë¶€ ìŠ¤í¬ë¡¤ ===\n",
    "        try:\n",
    "            scroll_target = driver.find_element(By.CSS_SELECTOR, 'div.css-1ulzvpi')\n",
    "            for _ in range(5):\n",
    "                ActionChains(driver).move_to_element(scroll_target).click().perform()\n",
    "                driver.execute_script(\"arguments[0].scrollTop += 500;\", scroll_target)\n",
    "                time.sleep(1)\n",
    "            print(\"âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(\"âŒ ìŠ¤í¬ë¡¤ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "        # === ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì •ë³´ ì¶”ì¶œ ===\n",
    "        checkin, checkout = \"ì •ë³´ ì—†ìŒ\", \"ì •ë³´ ì—†ìŒ\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//h3[contains(text(), 'ì²´í¬ì¸')]/following-sibling::div\"))\n",
    "            )\n",
    "            checkin_raw = driver.find_element(By.XPATH, \"//h3[contains(text(), 'ì²´í¬ì¸')]/following-sibling::div\")\n",
    "            checkout_raw = driver.find_element(By.XPATH, \"//h3[contains(text(), 'ì²´í¬ì•„ì›ƒ')]/following-sibling::div\")\n",
    "\n",
    "            # âœ… text ëŒ€ì‹  textContent\n",
    "            checkin = checkin_raw.get_attribute(\"textContent\").strip()\n",
    "            checkout = checkout_raw.get_attribute(\"textContent\").strip()\n",
    "            print(f\"âœ… ì²´í¬ì¸: {checkin} / ì²´í¬ì•„ì›ƒ: {checkout}\")\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì¶”ì¶œ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "        result.append({\n",
    "            \"LDGS_CD\": pid,\n",
    "            \"ì²´í¬ì¸\": checkin,\n",
    "            \"ì²´í¬ì•„ì›ƒ\": checkout\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "# === 5. ì‹¤í–‰ ===\n",
    "df_result = ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, ids)\n",
    "\n",
    "# === 6. ì¶œë ¥ ë° ì €ì¥ ===\n",
    "print(\"\\nğŸ“¦ ìµœì¢… ê²°ê³¼:\")\n",
    "print(df_result)\n",
    "df_result.to_csv(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_íœì…˜.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ba954",
   "metadata": {},
   "source": [
    "- ìµœì¢… ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ef9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ë° í•„í„°ë§ ===\n",
    "df_ì²´í¬ = pd.read_csv('0807ì²´í¬ì¸ì•„ì›ƒëˆ„ë½v1.csv')\n",
    "df_íœì…˜ = df_ì²´í¬[df_ì²´í¬['LDGMNT_TY_NM'] == 'Pension']\n",
    "df_íœì…˜['ìˆ™ì†ŒID'] = df_íœì…˜['ìˆ™ì†ŒID'].astype(str)\n",
    "\n",
    "# === 4. ì´ë¯¸ ìˆ˜ì§‘ëœ ìˆ™ì†Œ ì œê±° (ì¤‘ë‹¨ í›„ ì¬ì‹œì‘ìš©) ===\n",
    "completed_ids = []\n",
    "if os.path.exists(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_íœì…˜.csv\"):\n",
    "    completed_df = pd.read_csv(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_íœì…˜.csv\")\n",
    "    completed_ids = completed_df['ìˆ™ì†ŒID'].astype(str).tolist()\n",
    "\n",
    "# ìˆ˜ì§‘ ëŒ€ìƒ ID ë¦¬ìŠ¤íŠ¸\n",
    "target_ids = df_íœì…˜[~df_íœì…˜['ìˆ™ì†ŒID'].isin(completed_ids)]['ìˆ™ì†ŒID'].tolist()\n",
    "print(f\"\\nâœ… ì „ì²´ ëŒ€ìƒ: {len(df_íœì…˜)}ê°œ | ë‚¨ì€ ìˆ˜ì§‘ ëŒ€ìƒ: {len(target_ids)}ê°œ\\n\")\n",
    "\n",
    "# === 5. í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ ===\n",
    "def ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, id_list, save_every=50):\n",
    "    result = []\n",
    "    fail_log = []\n",
    "\n",
    "    for idx, pid in enumerate(id_list):\n",
    "        print(f\"\\n[{idx+1}/{len(id_list)}] ìˆ™ì†Œ ID {pid} ì ‘ì† ì¤‘...\")\n",
    "        driver.get(f\"https://place-site.yanolja.com/places/{pid}\")\n",
    "        time.sleep(2.5)\n",
    "\n",
    "        # ì „ì²´ë³´ê¸° ë²„íŠ¼ í´ë¦­\n",
    "        try:\n",
    "            btn = driver.find_element(By.CSS_SELECTOR, 'button.css-ff1hhr')\n",
    "            btn.click()\n",
    "            time.sleep(1.5)\n",
    "            print(\"âœ… ì „ì²´ë³´ê¸° í´ë¦­ ì„±ê³µ\")\n",
    "        except:\n",
    "            print(\"âŒ ì „ì²´ë³´ê¸° ë²„íŠ¼ ì—†ìŒ\")\n",
    "\n",
    "        # iframe ìŠ¤í¬ë¡¤\n",
    "        try:\n",
    "            scroll_target = driver.find_element(By.CSS_SELECTOR, 'div.css-1ulzvpi')\n",
    "            for _ in range(5):\n",
    "                ActionChains(driver).move_to_element(scroll_target).click().perform()\n",
    "                driver.execute_script(\"arguments[0].scrollTop += 500;\", scroll_target)\n",
    "                time.sleep(1)\n",
    "            print(\"âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(\"âŒ ìŠ¤í¬ë¡¤ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "        # ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì¶”ì¶œ\n",
    "        checkin, checkout = \"ì •ë³´ ì—†ìŒ\", \"ì •ë³´ ì—†ìŒ\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//h3[contains(text(), 'ì²´í¬ì¸')]/following-sibling::div\"))\n",
    "            )\n",
    "            checkin_raw = driver.find_element(By.XPATH, \"//h3[contains(text(), 'ì²´í¬ì¸')]/following-sibling::div\")\n",
    "            checkout_raw = driver.find_element(By.XPATH, \"//h3[contains(text(), 'ì²´í¬ì•„ì›ƒ')]/following-sibling::div\")\n",
    "            checkin = checkin_raw.get_attribute(\"textContent\").strip()\n",
    "            checkout = checkout_raw.get_attribute(\"textContent\").strip()\n",
    "            print(f\"âœ… ì²´í¬ì¸: {checkin} / ì²´í¬ì•„ì›ƒ: {checkout}\")\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì¶”ì¶œ ì‹¤íŒ¨:\", e)\n",
    "            fail_log.append({\"LDGS_CD\": pid, \"ì´ìœ \": \"ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì—†ìŒ\"})\n",
    "\n",
    "        result.append({\n",
    "            \"LDGS_CD\": pid,\n",
    "            \"ì²´í¬ì¸\": checkin,\n",
    "            \"ì²´í¬ì•„ì›ƒ\": checkout\n",
    "        })\n",
    "\n",
    "        # === 50ê°œ ë‹¨ìœ„ë¡œ ì €ì¥ ===\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(id_list):\n",
    "            temp_df = pd.DataFrame(result)\n",
    "            if os.path.exists(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_íœì…˜.csv\"):\n",
    "                temp_df.to_csv(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_íœì…˜.csv\", mode='a', header=False, index=False)\n",
    "            else:\n",
    "                temp_df.to_csv(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_íœì…˜.csv\", index=False)\n",
    "            print(f\"\\nğŸ’¾ ì €ì¥ ì™„ë£Œ (ëˆ„ì  {idx+1}ê°œ)\\n\")\n",
    "            result.clear()  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "\n",
    "    # ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥\n",
    "    if fail_log:\n",
    "        pd.DataFrame(fail_log).to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§_ì‹¤íŒ¨ë¡œê·¸.csv\", index=False)\n",
    "        print(f\"ğŸš¨ ì‹¤íŒ¨ ë¡œê·¸ {len(fail_log)}ê±´ ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# === 6. ì‹¤í–‰ ===\n",
    "ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, target_ids)\n",
    "\n",
    "print(\"\\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317d047",
   "metadata": {},
   "source": [
    "- ëˆ„ë½ìœ¼ë¡œ ì¬í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96abccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options, version_main=138)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ë° ìƒìœ„ 10ê°œ ì¶”ì¶œ ===\n",
    "df = pd.read_csv('ì•¼ë†€ì_í¬ë¡¤ë§_ì‹¤íŒ¨ë¡œê·¸.csv')\n",
    "# df = df[df['LDGMNT_TY_NM'] == 'Pension']\n",
    "ids = df['LDGS_CD'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, id_list):\n",
    "    result = []\n",
    "\n",
    "    for idx, pid in enumerate(id_list):\n",
    "        print(f\"\\n[{idx+1}] ìˆ™ì†Œ ID {pid} ì ‘ì† ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            driver.get(f\"https://place-site.yanolja.com/places/{pid}\")\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            # === ìŠ¤í¬ë¡¤ ë‚´ë¦¬ê¸°\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "                time.sleep(0.8)\n",
    "\n",
    "            # === ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì¶”ì¶œ\n",
    "            checkin_time = \"ì •ë³´ ì—†ìŒ\"\n",
    "            checkout_time = \"ì •ë³´ ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(driver, 8).until(\n",
    "                    EC.presence_of_all_elements_located((By.TAG_NAME, \"span\"))\n",
    "                )\n",
    "                spans = driver.find_elements(By.TAG_NAME, \"span\")\n",
    "                span_texts = [s.text.strip() for s in spans if s.text.strip()]\n",
    "                print(\"ğŸ“Œ span_texts:\", span_texts[:10])\n",
    "\n",
    "                for i in range(len(span_texts) - 1):\n",
    "                    try:\n",
    "                        if \"ì²´í¬ì¸\" in span_texts[i] and \":\" in span_texts[i + 1]:\n",
    "                            checkin_time = span_texts[i + 1]\n",
    "                        if \"ì²´í¬ì•„ì›ƒ\" in span_texts[i] and \":\" in span_texts[i + 1]:\n",
    "                            checkout_time = span_texts[i + 1]\n",
    "                        if checkin_time != \"ì •ë³´ ì—†ìŒ\" and checkout_time != \"ì •ë³´ ì—†ìŒ\":\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "\n",
    "            except Exception as inner_e:\n",
    "                print(f\"âŒ [ë‚´ë¶€ íŒŒì‹± ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {inner_e}\")\n",
    "\n",
    "        except Exception as outer_e:\n",
    "            print(f\"âŒ [í˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {outer_e}\")\n",
    "            checkin_time = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "            checkout_time = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "\n",
    "        # âœ… ë¬´ì¡°ê±´ ê²°ê³¼ ì €ì¥\n",
    "        result.append({\n",
    "            \"LDGS_CD\": pid,\n",
    "            \"ì²´í¬ì¸\": checkin_time,\n",
    "            \"ì²´í¬ì•„ì›ƒ\": checkout_time\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "df_result = ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, ids)\n",
    "df_result.to_csv(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_íœì…˜ëˆ„ë½.csv\", index=False)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdfc2f3",
   "metadata": {},
   "source": [
    "## ê²Œí•˜ ì²´í¬ì¸ ì²´í¬ì•„ì›ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options, version_main=138)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ë° ìƒìœ„ 10ê°œ ì¶”ì¶œ ===\n",
    "df = pd.read_csv('0807ì²´í¬ì¸ì•„ì›ƒëˆ„ë½v1.csv')\n",
    "df = df[df['LDGMNT_TY_NM'] == 'GuestHouse']\n",
    "ids = df['ìˆ™ì†ŒID'].astype(str).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, id_list):\n",
    "    result = []\n",
    "\n",
    "    for idx, pid in enumerate(id_list):\n",
    "        print(f\"\\n[{idx+1}] ìˆ™ì†Œ ID {pid} ì ‘ì† ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            driver.get(f\"https://place-site.yanolja.com/places/{pid}\")\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            # === ìŠ¤í¬ë¡¤ ë‚´ë¦¬ê¸°\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "                time.sleep(0.8)\n",
    "\n",
    "            # === ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì¶”ì¶œ\n",
    "            checkin_time = \"ì •ë³´ ì—†ìŒ\"\n",
    "            checkout_time = \"ì •ë³´ ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(driver, 8).until(\n",
    "                    EC.presence_of_all_elements_located((By.TAG_NAME, \"span\"))\n",
    "                )\n",
    "                spans = driver.find_elements(By.TAG_NAME, \"span\")\n",
    "                span_texts = [s.text.strip() for s in spans if s.text.strip()]\n",
    "                print(\"ğŸ“Œ span_texts:\", span_texts[:10])\n",
    "\n",
    "                for i in range(len(span_texts) - 1):\n",
    "                    try:\n",
    "                        if \"ì²´í¬ì¸\" in span_texts[i] and \":\" in span_texts[i + 1]:\n",
    "                            checkin_time = span_texts[i + 1]\n",
    "                        if \"ì²´í¬ì•„ì›ƒ\" in span_texts[i] and \":\" in span_texts[i + 1]:\n",
    "                            checkout_time = span_texts[i + 1]\n",
    "                        if checkin_time != \"ì •ë³´ ì—†ìŒ\" and checkout_time != \"ì •ë³´ ì—†ìŒ\":\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "\n",
    "            except Exception as inner_e:\n",
    "                print(f\"âŒ [ë‚´ë¶€ íŒŒì‹± ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {inner_e}\")\n",
    "\n",
    "        except Exception as outer_e:\n",
    "            print(f\"âŒ [í˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {outer_e}\")\n",
    "            checkin_time = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "            checkout_time = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "\n",
    "        # âœ… ë¬´ì¡°ê±´ ê²°ê³¼ ì €ì¥\n",
    "        result.append({\n",
    "            \"LDGS_CD\": pid,\n",
    "            \"ì²´í¬ì¸\": checkin_time,\n",
    "            \"ì²´í¬ì•„ì›ƒ\": checkout_time\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "df_result = ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, ids)\n",
    "df_result.to_csv(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤.csv\", index=False)\n",
    "df_result.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa7f33",
   "metadata": {},
   "source": [
    "## í˜¸í…” ì²´í¬ì¸ ì²´í¬ì•„ì›ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bc170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "df = pd.read_csv('0807ì²´í¬ì¸ì•„ì›ƒëˆ„ë½v1.csv')\n",
    "df = df[df['LDGMNT_TY_NM'] == 'Hotel']\n",
    "ids = df['ìˆ™ì†ŒID'].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, id_list):\n",
    "    result = []\n",
    "\n",
    "    for idx, pid in enumerate(id_list):\n",
    "        print(f\"\\n[{idx+1}/{len(id_list)}] ìˆ™ì†Œ ID {pid} ì ‘ì† ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            driver.get(f\"https://place-site.yanolja.com/places/{pid}\")\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            # === ìŠ¤í¬ë¡¤ ë‚´ë¦¬ê¸°\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "                time.sleep(0.8)\n",
    "\n",
    "            # === ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì¶”ì¶œ\n",
    "            checkin_time = \"ì •ë³´ ì—†ìŒ\"\n",
    "            checkout_time = \"ì •ë³´ ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(driver, 8).until(\n",
    "                    EC.presence_of_all_elements_located((By.TAG_NAME, \"span\"))\n",
    "                )\n",
    "                spans = driver.find_elements(By.TAG_NAME, \"span\")\n",
    "                span_texts = [s.text.strip() for s in spans if s.text.strip()]\n",
    "                print(\"ğŸ“Œ span_texts:\", span_texts[:10])\n",
    "\n",
    "                for i in range(len(span_texts) - 1):\n",
    "                    try:\n",
    "                        if \"ì²´í¬ì¸\" in span_texts[i] and \":\" in span_texts[i + 1]:\n",
    "                            checkin_time = span_texts[i + 1]\n",
    "                        if \"ì²´í¬ì•„ì›ƒ\" in span_texts[i] and \":\" in span_texts[i + 1]:\n",
    "                            checkout_time = span_texts[i + 1]\n",
    "                        if checkin_time != \"ì •ë³´ ì—†ìŒ\" and checkout_time != \"ì •ë³´ ì—†ìŒ\":\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "\n",
    "            except Exception as inner_e:\n",
    "                print(f\"âŒ [ë‚´ë¶€ íŒŒì‹± ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {inner_e}\")\n",
    "\n",
    "        except Exception as outer_e:\n",
    "            print(f\"âŒ [í˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {outer_e}\")\n",
    "            checkin_time = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "            checkout_time = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "\n",
    "        # âœ… ë¬´ì¡°ê±´ ê²°ê³¼ ì €ì¥\n",
    "        result.append({\n",
    "            \"LDGS_CD\": pid,\n",
    "            \"ì²´í¬ì¸\": checkin_time,\n",
    "            \"ì²´í¬ì•„ì›ƒ\": checkout_time\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "df_result = ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, ids)\n",
    "df_result.to_csv(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_í˜¸í…”.csv\", index=False)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106396b",
   "metadata": {},
   "source": [
    "- ëˆ„ë½ ê°’ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "df = pd.read_csv('ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_í˜¸í…”ëˆ„ë½.csv')\n",
    "# df = df[df['LDGMNT_TY_NM'] == 'Hotel']\n",
    "ids = df['LDGS_CD'].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "df = pd.read_csv('ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_í˜¸í…”ëˆ„ë½.csv')\n",
    "# df = df[df['LDGMNT_TY_NM'] == 'Hotel']\n",
    "ids = df['LDGS_CD'].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be94b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82271da0",
   "metadata": {},
   "source": [
    "## ëª¨í…” ì²´í¬ì¸ì•„ì›ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "df = pd.read_csv('0807ì²´í¬ì¸ì•„ì›ƒëˆ„ë½v1.csv')\n",
    "df = df[df['LDGMNT_TY_NM'] == 'Motel']\n",
    "ids = df['ìˆ™ì†ŒID'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, id_list):\n",
    "    result = []\n",
    "\n",
    "    for idx, pid in enumerate(id_list):\n",
    "        print(f\"\\n[{idx+1}/{len(id_list)}] ìˆ™ì†Œ ID {pid} ì ‘ì† ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            driver.get(f\"https://place-site.yanolja.com/places/{pid}\")\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            # === ìŠ¤í¬ë¡¤ ë‚´ë¦¬ê¸°\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "                time.sleep(0.8)\n",
    "\n",
    "            # === ì²´í¬ì¸/ì²´í¬ì•„ì›ƒ ì¶”ì¶œ (ìˆ™ë°• ì˜ì—­ë§Œ)\n",
    "            checkin_time = \"ì •ë³´ ì—†ìŒ\"\n",
    "            checkout_time = \"ì •ë³´ ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                # ìˆ™ë°• ê´€ë ¨ ìš”ê¸ˆì œ ë¸”ë¡ ì°¾ê¸°\n",
    "                containers = driver.find_elements(By.CLASS_NAME, \"rate-plan-container\")\n",
    "                found = False\n",
    "\n",
    "                for container in containers:\n",
    "                    if \"ìˆ™ë°•\" in container.text:\n",
    "                        try:\n",
    "                            spans = container.find_elements(By.TAG_NAME, \"span\")\n",
    "                            span_texts = [s.text.strip() for s in spans if s.text.strip()]\n",
    "                            print(\"ğŸ“Œ ìˆ™ë°• span_texts:\", span_texts[:10])\n",
    "\n",
    "                            for i in range(len(span_texts) - 1):\n",
    "                                if \"ì²´í¬ì¸\" in span_texts[i] and \":\" in span_texts[i + 1]:\n",
    "                                    checkin_time = span_texts[i + 1]\n",
    "                                if \"ì²´í¬ì•„ì›ƒ\" in span_texts[i] and \":\" in span_texts[i + 1]:\n",
    "                                    checkout_time = span_texts[i + 1]\n",
    "                                if checkin_time != \"ì •ë³´ ì—†ìŒ\" and checkout_time != \"ì •ë³´ ì—†ìŒ\":\n",
    "                                    found = True\n",
    "                                    break\n",
    "                            if found:\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "            except Exception as inner_e:\n",
    "                print(f\"âŒ [ë‚´ë¶€ íŒŒì‹± ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {inner_e}\")\n",
    "\n",
    "        except Exception as outer_e:\n",
    "            print(f\"âŒ [í˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {outer_e}\")\n",
    "            checkin_time = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "            checkout_time = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "\n",
    "        # âœ… ë¬´ì¡°ê±´ ê²°ê³¼ ì €ì¥\n",
    "        result.append({\n",
    "            \"LDGS_CD\": pid,\n",
    "            \"ì²´í¬ì¸\": checkin_time,\n",
    "            \"ì²´í¬ì•„ì›ƒ\": checkout_time\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "# === ì‹¤í–‰ ===\n",
    "df_result = ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ(driver, ids)\n",
    "df_result.to_csv(\"ì•¼ë†€ì_ì²´í¬ì¸ì•„ì›ƒ_ëª¨í…”.csv\", index=False)\n",
    "df_result.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e6112",
   "metadata": {},
   "source": [
    "# ì•¼ë†€ì ê°€ê²© ë° ì •ë³´ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "all_product_CD = pd.read_csv('ì²´í¬ì¸ì•„ì›ƒëˆ„ë½.csv')\n",
    "\n",
    "\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(0.4)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°\n",
    "\n",
    "driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "time.sleep(0.4)\n",
    "driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[2]/div/table/tbody/tr[2]/td[7]').click()\n",
    "time.sleep(0.4)\n",
    "driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[2]/div/table/tbody/tr[3]/td[1]').click()\n",
    "time.sleep(0.4)\n",
    "driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "\n",
    "\n",
    "# search_input = driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input')\n",
    "# time.sleep(0.8)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°\n",
    "\n",
    "# search_input.send_keys('ê´´ì‚° ë”ì§„íœì…˜')\n",
    "# search_input.send_keys(Keys.ENTER)\n",
    "# time.sleep(1.2)\n",
    "# driver.find_element(By.XPATH , value = '/html/body/div[1]/div/div[2]/div/div[1]/a/div/div[2]/div[1]/p').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('ë©”ì¸í”„ë ˆì„.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780b61f",
   "metadata": {},
   "source": [
    "## ì£¼ì¤‘íŒë§¤ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53191f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_total.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_log.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result = []\n",
    "    failed_ids = []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click()\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_element(By.CLASS_NAME, 'typography-body-14-bold').text.strip()\n",
    "            except:\n",
    "                rating = \"ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                review_count = driver.find_element(By.XPATH, '//p[@class=\"flex items-center justify-start gap-2 pl-2\"]/span[2]').text.strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            except:\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "\n",
    "            ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "            try:\n",
    "                ìˆ™ë°•_í…ìŠ¤íŠ¸ = soup.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t)\n",
    "                if ìˆ™ë°•_í…ìŠ¤íŠ¸:\n",
    "                    ìˆ™ë°•_ë°•ìŠ¤ = ìˆ™ë°•_í…ìŠ¤íŠ¸.find_parent(\"div\").find_next_sibling(\"div\")\n",
    "                    ì£¼ì¤‘íŒë§¤ê°€ = ìˆ™ë°•_ë°•ìŠ¤.text.strip().replace(\",\", \"\").replace(\"ì›~\", \"\").replace(\"ì›\", \"\")\n",
    "            except Exception as e:\n",
    "                print(f\" ìˆ™ë°•ê°€ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ë³„ì \": rating,\n",
    "                \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "                \"ì£¼ì¤‘íŒë§¤ê°€\": ì£¼ì¤‘íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° ì‹¤íŒ¨ or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "\n",
    "            # ì‹¤íŒ¨í•œ IDë¥¼ CSVì— ì €ì¥\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path,\n",
    "                mode='a',\n",
    "                index=False,\n",
    "                header=not os.path.exists(failed_path),\n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            driver = init_driver()\n",
    "            url = 'https://nol.yanolja.com/search?tab=place'\n",
    "            driver.get(url)\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "           \n",
    "                       # âœ… ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì‹œ ë‚ ì§œ ì¬ì„ íƒ\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[4]/td[3]').click()\n",
    "                time.sleep(0.4)                           \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[4]/td[4]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "                continue  # ë‚ ì§œ í´ë¦­ë„ ì‹¤íŒ¨í–ˆìœ¼ë©´ ë‹¤ìŒ IDë¡œ ë„˜ê¹€\n",
    "           \n",
    "            continue\n",
    "\n",
    "        if len(result) >= save_every:\n",
    "            df_temp = pd.DataFrame(result)\n",
    "            df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    if result:\n",
    "        df_temp = pd.DataFrame(result)\n",
    "        df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cae196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ë©”ì¸í”„ë ˆì„.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['LDGS_CD'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´, df_ì‹¤íŒ¨ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/ìˆ™ì†Œì •ë³´.csv\", index=False)\n",
    "df_ì‹¤íŒ¨.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/ì‹¤íŒ¨ëª©ë¡.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027f756",
   "metadata": {},
   "source": [
    "## 1ì°¨ ì£¼ì¤‘íŒë§¤ê°€ ëˆ„ë½ ìƒí’ˆID í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa86fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71badcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_ì£¼ì¤‘ëˆ„ë½.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_ì£¼ì¤‘ëˆ„ë½.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result = []\n",
    "    failed_ids = []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click()\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_element(By.CLASS_NAME, 'typography-body-14-bold').text.strip()\n",
    "            except:\n",
    "                rating = \"ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                review_count = driver.find_element(By.XPATH, '//p[@class=\"flex items-center justify-start gap-2 pl-2\"]/span[2]').text.strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            except:\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "\n",
    "            ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "            try:\n",
    "                ìˆ™ë°•_í…ìŠ¤íŠ¸ = soup.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t)\n",
    "                if ìˆ™ë°•_í…ìŠ¤íŠ¸:\n",
    "                    ìˆ™ë°•_ë°•ìŠ¤ = ìˆ™ë°•_í…ìŠ¤íŠ¸.find_parent(\"div\").find_next_sibling(\"div\")\n",
    "                    ì£¼ì¤‘íŒë§¤ê°€ = ìˆ™ë°•_ë°•ìŠ¤.text.strip().replace(\",\", \"\").replace(\"ì›~\", \"\").replace(\"ì›\", \"\")\n",
    "            except Exception as e:\n",
    "                print(f\" ìˆ™ë°•ê°€ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ë³„ì \": rating,\n",
    "                \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "                \"ì£¼ì¤‘íŒë§¤ê°€\": ì£¼ì¤‘íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° ì‹¤íŒ¨ or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "\n",
    "            # ì‹¤íŒ¨í•œ IDë¥¼ CSVì— ì €ì¥\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path,\n",
    "                mode='a',\n",
    "                index=False,\n",
    "                header=not os.path.exists(failed_path),\n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            driver = init_driver()\n",
    "            url = 'https://nol.yanolja.com/search?tab=place'\n",
    "            driver.get(url)\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "           \n",
    "                       # âœ… ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì‹œ ë‚ ì§œ ì¬ì„ íƒ\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                        \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[3]/td[3]').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[3]/td[4]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "                continue  # ë‚ ì§œ í´ë¦­ë„ ì‹¤íŒ¨í–ˆìœ¼ë©´ ë‹¤ìŒ IDë¡œ ë„˜ê¹€\n",
    "           \n",
    "            continue\n",
    "\n",
    "        if len(result) >= save_every:\n",
    "            df_temp = pd.DataFrame(result)\n",
    "            df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    if result:\n",
    "        df_temp = pd.DataFrame(result)\n",
    "        df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ëˆ„ë½ëœ_ìƒí’ˆID_í¬ë¡¤ë§.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['ìƒí’ˆID'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´, df_ì‹¤íŒ¨ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/ëˆ„ë½ìˆ™ì†Œ.csv\", index=False)\n",
    "df_ì‹¤íŒ¨.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/ëˆ„ë½ì‹¤íŒ¨.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aded9f8",
   "metadata": {},
   "source": [
    "## 2ì°¨ ì£¼ì¤‘íŒë§¤ê°€ ëˆ„ë½ ìƒí’ˆID í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_2ì°¨ì£¼ì¤‘ëˆ„ë½.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_2ì°¨ì£¼ì¤‘ëˆ„ë½.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result = []\n",
    "    failed_ids = []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click()\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_element(By.CLASS_NAME, 'typography-body-14-bold').text.strip()\n",
    "            except:\n",
    "                rating = \"ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                review_count = driver.find_element(By.XPATH, '//p[@class=\"flex items-center justify-start gap-2 pl-2\"]/span[2]').text.strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            except:\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "\n",
    "            ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "            try:\n",
    "                ìˆ™ë°•_í…ìŠ¤íŠ¸ = soup.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t)\n",
    "                if ìˆ™ë°•_í…ìŠ¤íŠ¸:\n",
    "                    ìˆ™ë°•_ë°•ìŠ¤ = ìˆ™ë°•_í…ìŠ¤íŠ¸.find_parent(\"div\").find_next_sibling(\"div\")\n",
    "                    ì£¼ì¤‘íŒë§¤ê°€ = ìˆ™ë°•_ë°•ìŠ¤.text.strip().replace(\",\", \"\").replace(\"ì›~\", \"\").replace(\"ì›\", \"\")\n",
    "            except Exception as e:\n",
    "                print(f\" ìˆ™ë°•ê°€ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ë³„ì \": rating,\n",
    "                \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "                \"ì£¼ì¤‘íŒë§¤ê°€\": ì£¼ì¤‘íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° ì‹¤íŒ¨ or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "\n",
    "            # ì‹¤íŒ¨í•œ IDë¥¼ CSVì— ì €ì¥\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path,\n",
    "                mode='a',\n",
    "                index=False,\n",
    "                header=not os.path.exists(failed_path),\n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            driver = init_driver()\n",
    "            url = 'https://nol.yanolja.com/search?tab=place'\n",
    "            driver.get(url)\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "           \n",
    "                       # âœ… ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì‹œ ë‚ ì§œ ì¬ì„ íƒ\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                        \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[5]').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[6]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "                continue  # ë‚ ì§œ í´ë¦­ë„ ì‹¤íŒ¨í–ˆìœ¼ë©´ ë‹¤ìŒ IDë¡œ ë„˜ê¹€\n",
    "           \n",
    "            continue\n",
    "\n",
    "        if len(result) >= save_every:\n",
    "            df_temp = pd.DataFrame(result)\n",
    "            df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    if result:\n",
    "        df_temp = pd.DataFrame(result)\n",
    "        df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf34b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"2ì°¨ëˆ„ë½í¬ë¡¤ë§.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['ìˆ™ì†ŒID'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´, df_ì‹¤íŒ¨ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/2ì°¨ëˆ„ë½ìˆ™ì†Œ.csv\", index=False)\n",
    "df_ì‹¤íŒ¨.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/2ì°¨ëˆ„ë½ì‹¤íŒ¨.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa334806",
   "metadata": {},
   "source": [
    "## ì£¼ë§ ê°€ê²©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_total_ì£¼ë§.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_log_ì£¼ë§.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result = []\n",
    "    failed_ids = []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click()\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_element(By.CLASS_NAME, 'typography-body-14-bold').text.strip()\n",
    "            except:\n",
    "                rating = \"ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                review_count = driver.find_element(By.XPATH, '//p[@class=\"flex items-center justify-start gap-2 pl-2\"]/span[2]').text.strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            except:\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "\n",
    "            ì£¼ë§íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "            try:\n",
    "                ìˆ™ë°•_í…ìŠ¤íŠ¸ = soup.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t)\n",
    "                if ìˆ™ë°•_í…ìŠ¤íŠ¸:\n",
    "                    ìˆ™ë°•_ë°•ìŠ¤ = ìˆ™ë°•_í…ìŠ¤íŠ¸.find_parent(\"div\").find_next_sibling(\"div\")\n",
    "                    ì£¼ë§íŒë§¤ê°€ = ìˆ™ë°•_ë°•ìŠ¤.text.strip().replace(\",\", \"\").replace(\"ì›~\", \"\").replace(\"ì›\", \"\")\n",
    "            except Exception as e:\n",
    "                print(f\" ìˆ™ë°•ê°€ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ë³„ì \": rating,\n",
    "                \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "                \"ì£¼ë§íŒë§¤ê°€\": ì£¼ë§íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° ì‹¤íŒ¨ or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "\n",
    "            # ì‹¤íŒ¨í•œ IDë¥¼ CSVì— ì €ì¥\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path,\n",
    "                mode='a',\n",
    "                index=False,\n",
    "                header=not os.path.exists(failed_path),\n",
    "                encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            driver = init_driver()\n",
    "            url = 'https://nol.yanolja.com/search?tab=place'\n",
    "            driver.get(url)\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "           \n",
    "                       # âœ… ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì‹œ ë‚ ì§œ ì¬ì„ íƒ\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[3]/td[7]').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[4]/td[1]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "                continue  # ë‚ ì§œ í´ë¦­ë„ ì‹¤íŒ¨í–ˆìœ¼ë©´ ë‹¤ìŒ IDë¡œ ë„˜ê¹€\n",
    "           \n",
    "            continue\n",
    "\n",
    "        if len(result) >= save_every:\n",
    "            df_temp = pd.DataFrame(result)\n",
    "            df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    if result:\n",
    "        df_temp = pd.DataFrame(result)\n",
    "        df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ë©”ì¸í”„ë ˆì„.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['LDGS_CD'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´_ì£¼ë§, df_ì‹¤íŒ¨_ì£¼ë§ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´_ì£¼ë§.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/ìˆ™ì†Œì •ë³´_ì£¼ë§.csv\", index=False)\n",
    "df_ì‹¤íŒ¨_ì£¼ë§.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/ì‹¤íŒ¨ëª©ë¡_ì£¼ë§.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´_ì£¼ë§.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cc701",
   "metadata": {},
   "source": [
    "## ì½”ë“œ ì˜¤ë¥˜ë¡œ ìˆ˜ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ae62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(random.uniform(2.5, 4.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2444d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_ê²°ê³¼/results_total.csv',\n",
    "                     failed_path='ì•¼ë†€ì_ê²°ê³¼/failed_log.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\"ğŸ”„ ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result = []\n",
    "    failed_ids = []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\"ğŸ¯ ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            # â›” ë“œë¼ì´ë²„ ì •ìƒ ì‘ë™ ì—¬ë¶€ ì ê²€\n",
    "            if not driver.title or \"ì•¼ë†€ì\" not in driver.title:\n",
    "                raise Exception(\"âš ï¸ ë“œë¼ì´ë²„ ë¹„ì •ìƒ ìƒíƒœ ê°ì§€ (title ì—†ìŒ ë˜ëŠ” ì•¼ë†€ì ë¯¸í¬í•¨)\")\n",
    "\n",
    "            # ğŸ” ê²€ìƒ‰ì°½ í´ë¦­ ë° ì…ë ¥\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click()\n",
    "            time.sleep(0.3)\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(random.uniform(2.5, 3.5))\n",
    "\n",
    "            # ê²°ê³¼ ìœ ë¬´ ì²´í¬\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\"â— ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_element(By.CLASS_NAME, 'typography-body-14-bold').text.strip()\n",
    "            except:\n",
    "                rating = \"ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                review_count = driver.find_element(By.XPATH, '//p[@class=\"flex items-center justify-start gap-2 pl-2\"]/span[2]').text.strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            except:\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "\n",
    "            ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "            try:\n",
    "                ìˆ™ë°•_í…ìŠ¤íŠ¸ = soup.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t)\n",
    "                if ìˆ™ë°•_í…ìŠ¤íŠ¸:\n",
    "                    ìˆ™ë°•_ë°•ìŠ¤ = ìˆ™ë°•_í…ìŠ¤íŠ¸.find_parent(\"div\").find_next_sibling(\"div\")\n",
    "                    ì£¼ì¤‘íŒë§¤ê°€ = ìˆ™ë°•_ë°•ìŠ¤.text.strip().replace(\",\", \"\").replace(\"ì›~\", \"\").replace(\"ì›\", \"\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ìˆ™ë°•ê°€ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ë³„ì \": rating,\n",
    "                \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "                \"ì£¼ì¤‘íŒë§¤ê°€\": ì£¼ì¤‘íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\"âœ… ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            # ë’¤ë¡œê°€ê¸°\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(random.uniform(1.0, 1.5))\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° ì‹¤íŒ¨ or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ğŸ›‘ ì—ëŸ¬ ë°œìƒ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "            \n",
    "\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(3)\n",
    "            driver = init_driver()  # ì™¸ë¶€ì— ì •ì˜ëœ í•¨ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
    "            continue\n",
    "\n",
    "        # ì¤‘ê°„ ì €ì¥\n",
    "        if len(result) >= save_every:\n",
    "            df_temp = pd.DataFrame(result)\n",
    "            df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\"ğŸ’¾ {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    # ë‚¨ì€ ê²°ê³¼ ì €ì¥\n",
    "    if result:\n",
    "        df_temp = pd.DataFrame(result)\n",
    "        df_temp.to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\"ğŸ’¾ ë§ˆì§€ë§‰ {len(df_temp)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2731558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ë©”ì¸í”„ë ˆì„.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['LDGS_CD'].astype(str).tolist()\n",
    "\n",
    "# âœ… í¬ë¡¤ë§ ì‹¤í–‰\n",
    "df_ì •ë³´ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "# âœ… ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_ê²°ê³¼/ìˆ™ì†Œì •ë³´.csv\", index=False)\n",
    "df_ì •ë³´.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bde124",
   "metadata": {},
   "source": [
    "- í™•ì¸ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "\n",
    "# âœ… ë“œë¼ì´ë²„ ì´ˆê¸°í™”\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options)\n",
    "\n",
    "# âœ… ë‚ ì§œ í´ë¦­ í•¨ìˆ˜\n",
    "def click_day(driver, aria_label):\n",
    "    xpath = f'//td[@aria-label=\"{aria_label}\"]'\n",
    "    element = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, xpath))).click()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# âœ… ë‚ ì§œ ì „ì²´ ì„¤ì • í•¨ìˆ˜\n",
    "def ë‚ ì§œ_ì„ íƒ(driver):\n",
    "    try:\n",
    "        # ë‚ ì§œ ë²„íŠ¼ í´ë¦­\n",
    "        date_btn_xpath = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button'\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, date_btn_xpath))).click()\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        # ìŠ¤í¬ë¡¤ ëŒ€ìƒ ì°¾ê¸° (ëª¨ë‹¬ ë‚´ë¶€)\n",
    "        scroll_target_xpath = '//div[contains(@class, \"DayPicker_transitionContainer_verticalScrollable\")]'\n",
    "        scrollable = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, scroll_target_xpath)))\n",
    "        driver.execute_script(\"arguments[0].scrollBy(0, 300);\", scrollable)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        # ë‚ ì§œ í´ë¦­\n",
    "        click_day(driver, \"Choose í™”ìš”ì¼, 2025ë…„ 8ì›” 19ì¼\")\n",
    "        click_day(driver, \"Choose ìˆ˜ìš”ì¼, 2025ë…„ 8ì›” 20ì¼\")\n",
    "\n",
    "        # í™•ì¸ í´ë¦­\n",
    "        confirm_xpath = '//button[contains(text(), \"í™•ì¸\")]'\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, confirm_xpath))).click()\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        print(\"âœ… ë‚ ì§œ ì„ íƒ ì™„ë£Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë‚ ì§œ ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "driver = init_driver()\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(random.uniform(2.5, 4.0))\n",
    "\n",
    "ë‚ ì§œ_ì„ íƒ(driver)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2726b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list):\n",
    "    result = []\n",
    "\n",
    "    for ìˆ™ì†Œ_id in ìˆ™ì†Œ_id_list:\n",
    "        print(f\"\\nâ–¶ ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "        try:\n",
    "            # ê²€ìƒ‰ì°½ì— ID ì…ë ¥\n",
    "            search_input = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input')\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            time.sleep(0.5)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            # ë³„ì \n",
    "            rating = driver.find_element(By.CLASS_NAME, 'typography-body-14-bold').text.strip()\n",
    "\n",
    "            # ë¦¬ë·° ìˆ˜\n",
    "            review_count_elem = driver.find_element(By.XPATH, '//p[@class=\"flex items-center justify-start gap-2 pl-2\"]/span[2]')\n",
    "            review_count = review_count_elem.text.strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "            # í• ì¸ë¥ \n",
    "            discount = driver.find_element(By.CLASS_NAME, 'ml-4.text-text-neutral-main.typography-body-12-regular').text.strip()\n",
    "\n",
    "            # ì •ê°€\n",
    "            original_price = driver.find_element(By.CLASS_NAME, 'ml-2.text-text-neutral-sub.line-through.typography-body-12-regular').text.strip().replace(\",\", \"\").replace(\"ì›\", \"\")\n",
    "\n",
    "            # íŒë§¤ê°€\n",
    "            price = driver.find_element(By.CLASS_NAME, 'shrink-0.grow-0.text-text-neutral-main.typography-subtitle-18-bold').text.strip().replace(\",\", \"\").replace(\"ì›\", \"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            rating = review_count = discount = original_price = price = \"ì •ë³´ ì—†ìŒ\"\n",
    "\n",
    "        result.append({\n",
    "            \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "            \"ë³„ì \": rating,\n",
    "            \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "            \"í• ì¸ë¥ \": discount,\n",
    "            \"ì •ê°€\": original_price,\n",
    "            \"íŒë§¤ê°€\": price\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33067724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ìˆ™ì†Œ ID ë¦¬ìŠ¤íŠ¸ ì •ì˜\n",
    "all_product_CD = ['1000114577']  # ë‹¤ë¥¸ ìˆ™ì†Œ IDë„ ì—¬ëŸ¬ ê°œ ì¶”ê°€ ê°€ëŠ¥\n",
    "\n",
    "# 2. í•¨ìˆ˜ í˜¸ì¶œ\n",
    "df_result = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, all_product_CD)\n",
    "\n",
    "# 3. ì¶œë ¥ ê²°ê³¼ í™•ì¸\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5489cff",
   "metadata": {},
   "source": [
    "# ì•¼ë†€ì ëˆ„ë½ ê°€ê²© í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options, version_main=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_2ì°¨ì£¼ì¤‘ëˆ„ë½.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_2ì°¨ì£¼ì¤‘ëˆ„ë½.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result, failed_ids = [], []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click(); time.sleep(0.3)\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # â­ï¸ ë³„ì  ì¶”ì¶œ\n",
    "            try:\n",
    "                rating_candidates = soup.find_all(\"span\", class_=\"typography-body-14-bold\")\n",
    "                rating = \"ì—†ìŒ\"\n",
    "                for span in rating_candidates:\n",
    "                    text = span.text.strip()\n",
    "                    if text.replace(\".\", \"\").isdigit():\n",
    "                        rating = text\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë³„ì  íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                rating = \"ì—†ìŒ\"\n",
    "\n",
    "            # â­ï¸ ë¦¬ë·° ìˆ˜ ì¶”ì¶œ (ë‘ êµ¬ì¡° ëª¨ë‘ ëŒ€ì‘)\n",
    "            try:\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "                \n",
    "                # 1ï¸âƒ£ ë³„ì  ì˜† êµ¬ì¡°: <span>5.0</span> <span>(3)</span>\n",
    "                rating_span = soup.find(\"span\", class_=\"typography-body-14-bold\")\n",
    "                if rating_span:\n",
    "                    p_tag = rating_span.find_parent(\"p\")\n",
    "                    span_tags = p_tag.find_all(\"span\")\n",
    "                    if len(span_tags) >= 2:\n",
    "                        second_span_text = span_tags[1].text.strip()\n",
    "                        if second_span_text.startswith(\"(\") and second_span_text.endswith(\")\"):\n",
    "                            review_count = second_span_text[1:-1]\n",
    "\n",
    "                # 2ï¸âƒ£ í›„ê¸° í…ìŠ¤íŠ¸ êµ¬ì¡°: 'í›„ê¸°' ë¬¸ìì—´ í¬í•¨í•œ span\n",
    "                if review_count == \"ì—†ìŒ\":\n",
    "                    review_span = soup.find(\"span\", string=lambda t: t and \"í›„ê¸°\" in t)\n",
    "                    if review_span:\n",
    "                        p_tag = review_span.find_parent(\"p\")\n",
    "                        span_tags = p_tag.find_all(\"span\")\n",
    "                        if len(span_tags) >= 2:\n",
    "                            review_text = span_tags[1].text.strip()\n",
    "                            review_count = review_text.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë¦¬ë·°ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "\n",
    "            # âœ… ì£¼ì¤‘íŒë§¤ê°€\n",
    "            ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "            try:\n",
    "                ìˆ™ë°•_í…ìŠ¤íŠ¸ = soup.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t)\n",
    "                if ìˆ™ë°•_í…ìŠ¤íŠ¸:\n",
    "                    ìˆ™ë°•_ë°•ìŠ¤ = ìˆ™ë°•_í…ìŠ¤íŠ¸.find_parent(\"div\").find_next_sibling(\"div\")\n",
    "                    ì£¼ì¤‘íŒë§¤ê°€ = ìˆ™ë°•_ë°•ìŠ¤.text.strip().replace(\",\", \"\").replace(\"ì›~\", \"\").replace(\"ì›\", \"\")\n",
    "            except: pass\n",
    "\n",
    "            # âœ… ì˜ˆì•½ë§ˆê°ì¼ ê²½ìš° 'ìˆ™ë°•' ë¸”ë¡ ë‚´ 'íŒë§¤ê°€' ì¶”ì¶œ\n",
    "            if ì£¼ì¤‘íŒë§¤ê°€ in [\"ì—†ìŒ\", \"ì˜ˆì•½ë§ˆê°\"]:\n",
    "                try:\n",
    "                    div_blocks = soup.find_all(\"div\", class_=\"mt-6 flex flex-wrap items-center justify-end\")\n",
    "                    for div in div_blocks:\n",
    "                        if div.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t):\n",
    "                            íŒë§¤ê°€_span = div.find(\"span\", string=lambda t: t and \"íŒë§¤ê°€\" in t)\n",
    "                            if íŒë§¤ê°€_span:\n",
    "                                ì£¼ì¤‘íŒë§¤ê°€ = (\n",
    "                                    íŒë§¤ê°€_span.text.strip()\n",
    "                                    .replace(\"íŒë§¤ê°€\", \"\")\n",
    "                                    .replace(\",\", \"\")\n",
    "                                    .replace(\"ì›~\", \"\")\n",
    "                                    .replace(\"ì›\", \"\")\n",
    "                                    .strip()\n",
    "                                )\n",
    "                                print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' ë¸”ë¡ ë‚´ íŒë§¤ê°€ ì¶”ì¶œ ì„±ê³µ â†’ {ì£¼ì¤‘íŒë§¤ê°€}\")\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' íŒë§¤ê°€ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ë³„ì \": rating,\n",
    "                \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "                \"ì£¼ì¤‘íŒë§¤ê°€\": ì£¼ì¤‘íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            # âœ… ê²€ìƒ‰ì°½ ì´ˆê¸°í™” & ë’¤ë¡œê°€ê¸°\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try: driver.quit()\n",
    "            except: pass\n",
    "\n",
    "            driver = init_driver()\n",
    "            driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                        \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[5]').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[6]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "            continue\n",
    "\n",
    "        # âœ… ì¤‘ê°„ ì €ì¥\n",
    "        if len(result) >= save_every:\n",
    "            pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    # âœ… ìµœì¢… ì €ì¥\n",
    "    if result:\n",
    "        pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ì•¼ë†€ì_2ì°¨ëˆ„ë½í¬ë¡¤ë§.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['ìˆ™ì†ŒID'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´, df_ì‹¤íŒ¨ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/2ì°¨ëˆ„ë½ìˆ™ì†Œ.csv\", index=False)\n",
    "df_ì‹¤íŒ¨.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/2ì°¨ëˆ„ë½ì‹¤íŒ¨.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2cddf5",
   "metadata": {},
   "source": [
    "## ì£¼ì¤‘ ëˆ„ë½ íŒë§¤ê°€ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options, version_main=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_3ì°¨ì£¼ì¤‘ëˆ„ë½.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_3ì°¨ì£¼ì¤‘ëˆ„ë½.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result, failed_ids = [], []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click(); time.sleep(0.3)\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # â­ï¸ ë³„ì  ì¶”ì¶œ\n",
    "            try:\n",
    "                rating_candidates = soup.find_all(\"span\", class_=\"typography-body-14-bold\")\n",
    "                rating = \"ì—†ìŒ\"\n",
    "                for span in rating_candidates:\n",
    "                    text = span.text.strip()\n",
    "                    if text.replace(\".\", \"\").isdigit():\n",
    "                        rating = text\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë³„ì  íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                rating = \"ì—†ìŒ\"\n",
    "\n",
    "            # â­ï¸ ë¦¬ë·° ìˆ˜ ì¶”ì¶œ (ë‘ êµ¬ì¡° ëª¨ë‘ ëŒ€ì‘)\n",
    "            try:\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "                \n",
    "                # 1ï¸âƒ£ ë³„ì  ì˜† êµ¬ì¡°: <span>5.0</span> <span>(3)</span>\n",
    "                rating_span = soup.find(\"span\", class_=\"typography-body-14-bold\")\n",
    "                if rating_span:\n",
    "                    p_tag = rating_span.find_parent(\"p\")\n",
    "                    span_tags = p_tag.find_all(\"span\")\n",
    "                    if len(span_tags) >= 2:\n",
    "                        second_span_text = span_tags[1].text.strip()\n",
    "                        if second_span_text.startswith(\"(\") and second_span_text.endswith(\")\"):\n",
    "                            review_count = second_span_text[1:-1]\n",
    "\n",
    "                # 2ï¸âƒ£ í›„ê¸° í…ìŠ¤íŠ¸ êµ¬ì¡°: 'í›„ê¸°' ë¬¸ìì—´ í¬í•¨í•œ span\n",
    "                if review_count == \"ì—†ìŒ\":\n",
    "                    review_span = soup.find(\"span\", string=lambda t: t and \"í›„ê¸°\" in t)\n",
    "                    if review_span:\n",
    "                        p_tag = review_span.find_parent(\"p\")\n",
    "                        span_tags = p_tag.find_all(\"span\")\n",
    "                        if len(span_tags) >= 2:\n",
    "                            review_text = span_tags[1].text.strip()\n",
    "                            review_count = review_text.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë¦¬ë·°ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "\n",
    "            # âœ… ì£¼ì¤‘íŒë§¤ê°€\n",
    "            ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "            try:\n",
    "                ìˆ™ë°•_í…ìŠ¤íŠ¸ = soup.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t)\n",
    "                if ìˆ™ë°•_í…ìŠ¤íŠ¸:\n",
    "                    ìˆ™ë°•_ë°•ìŠ¤ = ìˆ™ë°•_í…ìŠ¤íŠ¸.find_parent(\"div\").find_next_sibling(\"div\")\n",
    "                    ì£¼ì¤‘íŒë§¤ê°€ = ìˆ™ë°•_ë°•ìŠ¤.text.strip().replace(\",\", \"\").replace(\"ì›~\", \"\").replace(\"ì›\", \"\")\n",
    "            except: pass\n",
    "\n",
    "            # âœ… ì˜ˆì•½ë§ˆê°ì¼ ê²½ìš° 'ìˆ™ë°•' ë¸”ë¡ ë‚´ 'íŒë§¤ê°€' ì¶”ì¶œ\n",
    "            if ì£¼ì¤‘íŒë§¤ê°€ in [\"ì—†ìŒ\", \"ì˜ˆì•½ë§ˆê°\"]:\n",
    "                try:\n",
    "                    div_blocks = soup.find_all(\"div\", class_=\"mt-6 flex flex-wrap items-center justify-end\")\n",
    "                    for div in div_blocks:\n",
    "                        if div.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t):\n",
    "                            íŒë§¤ê°€_span = div.find(\"span\", string=lambda t: t and \"íŒë§¤ê°€\" in t)\n",
    "                            if íŒë§¤ê°€_span:\n",
    "                                ì£¼ì¤‘íŒë§¤ê°€ = (\n",
    "                                    íŒë§¤ê°€_span.text.strip()\n",
    "                                    .replace(\"íŒë§¤ê°€\", \"\")\n",
    "                                    .replace(\",\", \"\")\n",
    "                                    .replace(\"ì›~\", \"\")\n",
    "                                    .replace(\"ì›\", \"\")\n",
    "                                    .strip()\n",
    "                                )\n",
    "                                print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' ë¸”ë¡ ë‚´ íŒë§¤ê°€ ì¶”ì¶œ ì„±ê³µ â†’ {ì£¼ì¤‘íŒë§¤ê°€}\")\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' íŒë§¤ê°€ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ë³„ì \": rating,\n",
    "                \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "                \"ì£¼ì¤‘íŒë§¤ê°€\": ì£¼ì¤‘íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            # âœ… ê²€ìƒ‰ì°½ ì´ˆê¸°í™” & ë’¤ë¡œê°€ê¸°\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try: driver.quit()\n",
    "            except: pass\n",
    "\n",
    "            driver = init_driver()\n",
    "            driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                        \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[3]/td[2]').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[3]/td[3]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "            continue\n",
    "\n",
    "        # âœ… ì¤‘ê°„ ì €ì¥\n",
    "        if len(result) >= save_every:\n",
    "            pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    # âœ… ìµœì¢… ì €ì¥\n",
    "    if result:\n",
    "        pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ëˆ„ë½_ìˆ™ì†ŒID_ëª©ë¡.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['ìˆ™ì†ŒID'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´, df_ì‹¤íŒ¨ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/3ì°¨ëˆ„ë½ìˆ™ì†Œ.csv\", index=False)\n",
    "df_ì‹¤íŒ¨.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/3ì°¨ëˆ„ë½ì‹¤íŒ¨.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef20a7",
   "metadata": {},
   "source": [
    "## ì£¼ë§ ëˆ„ë½ìˆ™ì†Œ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options, version_main=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_1ì°¨ì£¼ë§ëˆ„ë½.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_1ì°¨ì£¼ë§ëˆ„ë½.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result, failed_ids = [], []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click(); time.sleep(0.3)\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # â­ï¸ ë³„ì  ì¶”ì¶œ\n",
    "            try:\n",
    "                rating_candidates = soup.find_all(\"span\", class_=\"typography-body-14-bold\")\n",
    "                rating = \"ì—†ìŒ\"\n",
    "                for span in rating_candidates:\n",
    "                    text = span.text.strip()\n",
    "                    if text.replace(\".\", \"\").isdigit():\n",
    "                        rating = text\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë³„ì  íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                rating = \"ì—†ìŒ\"\n",
    "\n",
    "            # â­ï¸ ë¦¬ë·° ìˆ˜ ì¶”ì¶œ (ë‘ êµ¬ì¡° ëª¨ë‘ ëŒ€ì‘)\n",
    "            try:\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "                \n",
    "                # 1ï¸âƒ£ ë³„ì  ì˜† êµ¬ì¡°: <span>5.0</span> <span>(3)</span>\n",
    "                rating_span = soup.find(\"span\", class_=\"typography-body-14-bold\")\n",
    "                if rating_span:\n",
    "                    p_tag = rating_span.find_parent(\"p\")\n",
    "                    span_tags = p_tag.find_all(\"span\")\n",
    "                    if len(span_tags) >= 2:\n",
    "                        second_span_text = span_tags[1].text.strip()\n",
    "                        if second_span_text.startswith(\"(\") and second_span_text.endswith(\")\"):\n",
    "                            review_count = second_span_text[1:-1]\n",
    "\n",
    "                # 2ï¸âƒ£ í›„ê¸° í…ìŠ¤íŠ¸ êµ¬ì¡°: 'í›„ê¸°' ë¬¸ìì—´ í¬í•¨í•œ span\n",
    "                if review_count == \"ì—†ìŒ\":\n",
    "                    review_span = soup.find(\"span\", string=lambda t: t and \"í›„ê¸°\" in t)\n",
    "                    if review_span:\n",
    "                        p_tag = review_span.find_parent(\"p\")\n",
    "                        span_tags = p_tag.find_all(\"span\")\n",
    "                        if len(span_tags) >= 2:\n",
    "                            review_text = span_tags[1].text.strip()\n",
    "                            review_count = review_text.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë¦¬ë·°ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                review_count = \"ì—†ìŒ\"\n",
    "\n",
    "            # âœ… ì£¼ì¤‘íŒë§¤ê°€\n",
    "            ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "            try:\n",
    "                ìˆ™ë°•_í…ìŠ¤íŠ¸ = soup.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t)\n",
    "                if ìˆ™ë°•_í…ìŠ¤íŠ¸:\n",
    "                    ìˆ™ë°•_ë°•ìŠ¤ = ìˆ™ë°•_í…ìŠ¤íŠ¸.find_parent(\"div\").find_next_sibling(\"div\")\n",
    "                    ì£¼ì¤‘íŒë§¤ê°€ = ìˆ™ë°•_ë°•ìŠ¤.text.strip().replace(\",\", \"\").replace(\"ì›~\", \"\").replace(\"ì›\", \"\")\n",
    "            except: pass\n",
    "\n",
    "            # âœ… ì˜ˆì•½ë§ˆê°ì¼ ê²½ìš° 'ìˆ™ë°•' ë¸”ë¡ ë‚´ 'íŒë§¤ê°€' ì¶”ì¶œ\n",
    "            if ì£¼ì¤‘íŒë§¤ê°€ in [\"ì—†ìŒ\", \"ì˜ˆì•½ë§ˆê°\"]:\n",
    "                try:\n",
    "                    div_blocks = soup.find_all(\"div\", class_=\"mt-6 flex flex-wrap items-center justify-end\")\n",
    "                    for div in div_blocks:\n",
    "                        if div.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t):\n",
    "                            íŒë§¤ê°€_span = div.find(\"span\", string=lambda t: t and \"íŒë§¤ê°€\" in t)\n",
    "                            if íŒë§¤ê°€_span:\n",
    "                                ì£¼ì¤‘íŒë§¤ê°€ = (\n",
    "                                    íŒë§¤ê°€_span.text.strip()\n",
    "                                    .replace(\"íŒë§¤ê°€\", \"\")\n",
    "                                    .replace(\",\", \"\")\n",
    "                                    .replace(\"ì›~\", \"\")\n",
    "                                    .replace(\"ì›\", \"\")\n",
    "                                    .strip()\n",
    "                                )\n",
    "                                print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' ë¸”ë¡ ë‚´ íŒë§¤ê°€ ì¶”ì¶œ ì„±ê³µ â†’ {ì£¼ì¤‘íŒë§¤ê°€}\")\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' íŒë§¤ê°€ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ë³„ì \": rating,\n",
    "                \"ë¦¬ë·°ìˆ˜\": review_count,\n",
    "                \"ì£¼ì¤‘íŒë§¤ê°€\": ì£¼ì¤‘íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            # âœ… ê²€ìƒ‰ì°½ ì´ˆê¸°í™” & ë’¤ë¡œê°€ê¸°\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try: driver.quit()\n",
    "            except: pass\n",
    "\n",
    "            driver = init_driver()\n",
    "            driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                        \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[7]').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[3]/td[1]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "            continue\n",
    "\n",
    "        # âœ… ì¤‘ê°„ ì €ì¥\n",
    "        if len(result) >= save_every:\n",
    "            pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    # âœ… ìµœì¢… ì €ì¥\n",
    "    if result:\n",
    "        pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/0806ì£¼ë§_ëˆ„ë½ìˆ™ì†Œ.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['ìˆ™ì†ŒID'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´, df_ì‹¤íŒ¨ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/1ì°¨ì£¼ë§ëˆ„ë½ìˆ™ì†Œ.csv\", index=False)\n",
    "df_ì‹¤íŒ¨.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/1ì°¨ì£¼ë§ëˆ„ë½ì‹¤íŒ¨.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7abcd0",
   "metadata": {},
   "source": [
    "## ë¹„ì •ìƒ_ì£¼ë§íŒë§¤ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options, version_main=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b895ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_3ì°¨ì£¼ë§ëˆ„ë½.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_3ì°¨ì£¼ë§ëˆ„ë½.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result, failed_ids = [], []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click(); time.sleep(0.3)\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # âœ… ì£¼ë§íŒë§¤ê°€ ì¶”ì¶œ (ë‘ ë²ˆì§¸ ê°€ê²©)\n",
    "            ì£¼ë§íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                price_tags = soup.find_all(\"span\", class_=\"shrink-0 grow-0 text-text-neutral-main typography-subtitle-18-bold\")\n",
    "                if len(price_tags) >= 2:\n",
    "                    price_text = price_tags[1].text.strip()\n",
    "                elif len(price_tags) == 1:\n",
    "                    price_text = price_tags[0].text.strip()\n",
    "                else:\n",
    "                    price_text = \"ì—†ìŒ\"\n",
    "\n",
    "                ì£¼ë§íŒë§¤ê°€ = (\n",
    "                    price_text.replace(\",\", \"\")\n",
    "                              .replace(\"ì›~\", \"\")\n",
    "                              .replace(\"ì›\", \"\")\n",
    "                              .strip()\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì£¼ë§íŒë§¤ê°€ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                ì£¼ë§íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "\n",
    "            # âœ… ì˜ˆì•½ë§ˆê° ì˜ˆì™¸ ì²˜ë¦¬\n",
    "            if ì£¼ë§íŒë§¤ê°€ in [\"ì—†ìŒ\", \"ì˜ˆì•½ë§ˆê°\"]:\n",
    "                try:\n",
    "                    div_blocks = soup.find_all(\"div\", class_=\"mt-6 flex flex-wrap items-center justify-end\")\n",
    "                    for div in div_blocks:\n",
    "                        if div.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t):\n",
    "                            íŒë§¤ê°€_span = div.find(\"span\", string=lambda t: t and \"íŒë§¤ê°€\" in t)\n",
    "                            if íŒë§¤ê°€_span:\n",
    "                                ì£¼ë§íŒë§¤ê°€ = (\n",
    "                                    íŒë§¤ê°€_span.text.strip()\n",
    "                                        .replace(\"íŒë§¤ê°€\", \"\")\n",
    "                                        .replace(\",\", \"\")\n",
    "                                        .replace(\"ì›~\", \"\")\n",
    "                                        .replace(\"ì›\", \"\")\n",
    "                                        .strip()\n",
    "                                )\n",
    "                                print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' ë¸”ë¡ ë‚´ íŒë§¤ê°€ ì¶”ì¶œ ì„±ê³µ â†’ {ì£¼ë§íŒë§¤ê°€}\")\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' íŒë§¤ê°€ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ì£¼ë§íŒë§¤ê°€\": ì£¼ë§íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            # âœ… ê²€ìƒ‰ì°½ ì´ˆê¸°í™” & ë’¤ë¡œê°€ê¸°\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try: driver.quit()\n",
    "            except: pass\n",
    "\n",
    "            driver = init_driver()\n",
    "            driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                        \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[6]').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[7]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "            continue\n",
    "\n",
    "        # âœ… ì¤‘ê°„ ì €ì¥\n",
    "        if len(result) >= save_every:\n",
    "            pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    # âœ… ìµœì¢… ì €ì¥\n",
    "    if result:\n",
    "        pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/ë¹„ì •ìƒ_ì£¼ë§íŒë§¤ê°€.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['ìˆ™ì†ŒID'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´, df_ì‹¤íŒ¨ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/1ë¹„ì •ìƒ_ì£¼ë§íŒë§¤ê°€.csv\", index=False)\n",
    "df_ì‹¤íŒ¨.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/1ë¹„ì •ìƒ_ì£¼ë§íŒë§¤ê°€ì‹¤íŒ¨.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e50c66",
   "metadata": {},
   "source": [
    "## ë¹„ì •ìƒ_ì£¼ì¤‘íŒë§¤ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8eb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def init_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "    return uc.Chrome(options=options, version_main=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afacbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = init_driver()\n",
    "\n",
    "url = 'https://nol.yanolja.com/search?tab=place'\n",
    "driver.get(url)\n",
    "time.sleep(random.uniform(2.5, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver,\n",
    "                     ìˆ™ì†Œ_id_list,\n",
    "                     save_every=50,\n",
    "                     result_path='ì•¼ë†€ì_í¬ë¡¤ë§/results_3ì°¨ì£¼ì¤‘ëˆ„ë½.csv',\n",
    "                     failed_path='ì•¼ë†€ì_í¬ë¡¤ë§/failed_3ì°¨ì£¼ì¤‘ëˆ„ë½.csv'):\n",
    "\n",
    "    os.makedirs(os.path.dirname(result_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(result_path):\n",
    "        saved_df = pd.read_csv(result_path)\n",
    "        saved_ids = set(saved_df['ìˆ™ì†ŒID'].astype(str).tolist())\n",
    "        print(f\" ê¸°ì¡´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}\")\n",
    "    else:\n",
    "        saved_ids = set()\n",
    "\n",
    "    result, failed_ids = [], []\n",
    "    is_first = not os.path.exists(result_path)\n",
    "    target_ids = [str(i) for i in ìˆ™ì†Œ_id_list if str(i) not in saved_ids]\n",
    "    print(f\" ìˆ˜ì§‘ ëŒ€ìƒ ID ìˆ˜: {len(target_ids)}\")\n",
    "\n",
    "    for idx, ìˆ™ì†Œ_id in enumerate(target_ids):\n",
    "        print(f\"\\nâ–¶ ({idx+1}/{len(target_ids)}) ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²€ìƒ‰ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "            )\n",
    "            search_input.click(); time.sleep(0.3)\n",
    "            search_input.clear()\n",
    "            search_input.send_keys(ìˆ™ì†Œ_id)\n",
    "            search_input.send_keys(Keys.ENTER)\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            cards = driver.find_elements(By.CLASS_NAME, 'relative')\n",
    "            if not cards:\n",
    "                print(f\" ìˆ™ì†Œ ID {ìˆ™ì†Œ_id} ê²°ê³¼ ì—†ìŒ\")\n",
    "                failed_ids.append(ìˆ™ì†Œ_id)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # âœ… ì£¼ì¤‘íŒë§¤ê°€ ì¶”ì¶œ (ë‘ ë²ˆì§¸ ê°€ê²©)\n",
    "            ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "\n",
    "            try:\n",
    "                price_tags = soup.find_all(\"span\", class_=\"shrink-0 grow-0 text-text-neutral-main typography-subtitle-18-bold\")\n",
    "                if len(price_tags) >= 2:\n",
    "                    price_text = price_tags[1].text.strip()\n",
    "                elif len(price_tags) == 1:\n",
    "                    price_text = price_tags[0].text.strip()\n",
    "                else:\n",
    "                    price_text = \"ì—†ìŒ\"\n",
    "\n",
    "                ì£¼ì¤‘íŒë§¤ê°€ = (\n",
    "                    price_text.replace(\",\", \"\")\n",
    "                              .replace(\"ì›~\", \"\")\n",
    "                              .replace(\"ì›\", \"\")\n",
    "                              .strip()\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì£¼ì¤‘íŒë§¤ê°€ íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                ì£¼ì¤‘íŒë§¤ê°€ = \"ì—†ìŒ\"\n",
    "\n",
    "            # âœ… ì˜ˆì•½ë§ˆê° ì˜ˆì™¸ ì²˜ë¦¬\n",
    "            if ì£¼ì¤‘íŒë§¤ê°€ in [\"ì—†ìŒ\", \"ì˜ˆì•½ë§ˆê°\"]:\n",
    "                try:\n",
    "                    div_blocks = soup.find_all(\"div\", class_=\"mt-6 flex flex-wrap items-center justify-end\")\n",
    "                    for div in div_blocks:\n",
    "                        if div.find(\"span\", string=lambda t: t and \"ìˆ™ë°•\" in t):\n",
    "                            íŒë§¤ê°€_span = div.find(\"span\", string=lambda t: t and \"íŒë§¤ê°€\" in t)\n",
    "                            if íŒë§¤ê°€_span:\n",
    "                                ì£¼ì¤‘íŒë§¤ê°€ = (\n",
    "                                    íŒë§¤ê°€_span.text.strip()\n",
    "                                        .replace(\"íŒë§¤ê°€\", \"\")\n",
    "                                        .replace(\",\", \"\")\n",
    "                                        .replace(\"ì›~\", \"\")\n",
    "                                        .replace(\"ì›\", \"\")\n",
    "                                        .strip()\n",
    "                                )\n",
    "                                print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' ë¸”ë¡ ë‚´ íŒë§¤ê°€ ì¶”ì¶œ ì„±ê³µ â†’ {ì£¼ì¤‘íŒë§¤ê°€}\")\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    print(f\"ğŸ“Œ ì˜ˆì•½ë§ˆê° â†’ 'ìˆ™ë°•' íŒë§¤ê°€ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "            result.append({\n",
    "                \"ìˆ™ì†ŒID\": ìˆ™ì†Œ_id,\n",
    "                \"ì£¼ì¤‘íŒë§¤ê°€\": ì£¼ì¤‘íŒë§¤ê°€\n",
    "            })\n",
    "            print(f\" ìˆ˜ì§‘ê²°ê³¼ â†’ {result[-1]}\")\n",
    "\n",
    "            # âœ… ê²€ìƒ‰ì°½ ì´ˆê¸°í™” & ë’¤ë¡œê°€ê¸°\n",
    "            try:\n",
    "                back_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"Back\" and @aria-disabled=\"false\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", back_button)\n",
    "                time.sleep(1.2)\n",
    "\n",
    "                search_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[1]/div/input'))\n",
    "                )\n",
    "                search_input.click()\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë’¤ë¡œê°€ê¸° or ê²€ìƒ‰ì°½ ì¬í™œì„±í™” ì‹¤íŒ¨ â†’ {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨ (ID: {ìˆ™ì†Œ_id}) â†’ {e}\")\n",
    "            failed_ids.append(ìˆ™ì†Œ_id)\n",
    "            pd.DataFrame({\"ìˆ™ì†ŒID\": [ìˆ™ì†Œ_id]}).to_csv(\n",
    "                failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig'\n",
    "            )\n",
    "\n",
    "            try: driver.quit()\n",
    "            except: pass\n",
    "\n",
    "            driver = init_driver()\n",
    "            driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "            time.sleep(random.uniform(2.5, 4.0))\n",
    "            try:\n",
    "                driver.find_element(By.XPATH , value = '//*[@id=\"__next\"]/div/main/div/div[1]/form/div/div[2]/div[1]/button').click()\n",
    "                time.sleep(0.4)                        \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[6]').click()\n",
    "                time.sleep(0.4)                         \n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[3]/div/div/div/div[2]/div/div[2]/div[1]/div/table/tbody/tr[2]/td[7]').click()\n",
    "                time.sleep(0.4)\n",
    "                driver.find_element(By.XPATH , value = '/html/body/div[4]/div/div/section/section[4]/button').click()\n",
    "                print(\"ğŸ” ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\" ë“œë¼ì´ë²„ ì¬ì‹œì‘ í›„ ë‚ ì§œ ì¬ì„ íƒ ì‹¤íŒ¨ â†’ {e}\")\n",
    "            continue\n",
    "\n",
    "        # âœ… ì¤‘ê°„ ì €ì¥\n",
    "        if len(result) >= save_every:\n",
    "            pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "            print(f\" {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "            result = []\n",
    "            is_first = False\n",
    "\n",
    "    # âœ… ìµœì¢… ì €ì¥\n",
    "    if result:\n",
    "        pd.DataFrame(result).to_csv(result_path, mode='a', header=is_first, index=False, encoding='utf-8-sig')\n",
    "        print(f\" ë§ˆì§€ë§‰ {len(result)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {result_path}\")\n",
    "\n",
    "    if failed_ids:\n",
    "        pd.DataFrame({\"ìˆ™ì†ŒID\": failed_ids}).to_csv(failed_path, mode='a', index=False, header=not os.path.exists(failed_path), encoding='utf-8-sig')\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨ ID {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ â†’ {failed_path}\")\n",
    "\n",
    "    print(\" ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ.\")\n",
    "    return pd.read_csv(result_path), pd.read_csv(failed_path) if os.path.exists(failed_path) else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a556a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ë©”ì¸í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/ë¹„ì •ìƒ_ì£¼ì¤‘íŒë§¤ê°€.csv\")\n",
    "ìˆ™ì†Œ_id_list = df['ìˆ™ì†ŒID'].astype(str).tolist()\n",
    "\n",
    "#  í¬ë¡¤ë§ ì‹¤í–‰ (í•¨ìˆ˜ì—ì„œ ê²°ê³¼ + ì‹¤íŒ¨ ë°˜í™˜)\n",
    "df_ì •ë³´, df_ì‹¤íŒ¨ = ì•¼ë†€ì_ìˆ™ì†Œì •ë³´_ì¶”ì¶œ(driver, ìˆ™ì†Œ_id_list)\n",
    "\n",
    "#  ìµœì¢… ì €ì¥\n",
    "df_ì •ë³´.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/1ë¹„ì •ìƒ_ì£¼ì¤‘íŒë§¤ê°€.csv\", index=False)\n",
    "df_ì‹¤íŒ¨.to_csv(\"ì•¼ë†€ì_í¬ë¡¤ë§/1ë¹„ì •ìƒ_ì£¼ì¤‘íŒë§¤ê°€ì‹¤íŒ¨.csv\", index=False)\n",
    "\n",
    "df_ì •ë³´.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026532a0",
   "metadata": {},
   "source": [
    "# ì£¼ë§ íŒë§¤ê°€ê²© ì˜¤ë¥˜ ìˆ˜ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. ë“œë¼ì´ë²„ ì„¤ì • ===\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.169 Safari/537.36\")\n",
    "driver = uc.Chrome(options=options, version_main=138)\n",
    "\n",
    "# === 2. ì§„ì…ìš© í˜ì´ì§€ ì—´ê¸° ===\n",
    "driver.get(\"https://nol.yanolja.com/search?tab=place\")\n",
    "time.sleep(0.4)\n",
    "\n",
    "# === 3. ìˆ™ì†Œ ID ë¶ˆëŸ¬ì˜¤ê¸° ===\n",
    "df = pd.read_csv('0814_íŒë§¤ê°€ë¹„ìœ¨1ë¯¸ë§Œ.csv')\n",
    "# ìˆ™ì†Œìš´ì˜í˜•íƒœ Motelì€ ì‚­ì œ\n",
    "df = df[df['ìˆ™ì†Œìš´ì˜í˜•íƒœ'] != 'Motel']\n",
    "ids = df['ìˆ™ì†ŒID'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def ì•¼ë†€ì_ì£¼ë§íŒë§¤ê°€(driver, id_list):\n",
    "    result = []\n",
    "\n",
    "    for idx, pid in enumerate(id_list):\n",
    "        print(f\"\\n[{idx+1}/{len(id_list)}] ìˆ™ì†Œ ID {pid} ì ‘ì† ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            driver.get(f\"https://place-site.yanolja.com/places/{pid}\")\n",
    "            time.sleep(2.5)\n",
    "\n",
    "            # ìŠ¤í¬ë¡¤ (í•„ìš”í•œ ê²½ìš°ë§Œ)\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "                time.sleep(0.8)\n",
    "\n",
    "            ì£¼ë§íŒë§¤ê°€ = \"ì •ë³´ ì—†ìŒ\"\n",
    "\n",
    "            # ìˆ™ë°• ìš”ê¸ˆì œ ì»¨í…Œì´ë„ˆ ì°¾ê¸°\n",
    "            containers = driver.find_elements(By.CLASS_NAME, \"rate-plan-container\")\n",
    "            for c in containers:\n",
    "                if \"ìˆ™ë°•\" not in c.text:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # ê°€ê²© ìŠ¤íŒ¬\n",
    "                    price_el = c.find_element(By.CSS_SELECTOR, \"span.price\")\n",
    "                    price_text = price_el.text.strip()\n",
    "\n",
    "                    # 'ì›' ë‹¨ìœ„ ìŠ¤íŒ¬ (ìˆì„ ê²½ìš°)\n",
    "                    unit_text = \"\"\n",
    "                    try:\n",
    "                        unit_el = c.find_element(By.CSS_SELECTOR, \"span.krw\")\n",
    "                        unit_text = unit_el.text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    ì£¼ë§íŒë§¤ê°€ = price_text + unit_text\n",
    "\n",
    "                    # â˜… 1) ë£¨í”„ ë‚´ ê°€ê²© ë¡œê·¸\n",
    "                    print(f\"   â†’ ê°€ê²© ìº¡ì²˜: {ì£¼ë§íŒë§¤ê°€}\")\n",
    "\n",
    "                    break  # ì²« ë²ˆì§¸ ìˆ™ë°• ê°€ê²©ë§Œ ì‚¬ìš©\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ [ì ‘ì† ì˜¤ë¥˜] ìˆ™ì†Œ ID {pid}: {e}\")\n",
    "            ì£¼ë§íŒë§¤ê°€ = \"ì ‘ì† ì˜¤ë¥˜\"\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        result.append({\n",
    "            \"ìˆ™ì†ŒID\": pid,\n",
    "            \"ì£¼ë§íŒë§¤ê°€\": ì£¼ë§íŒë§¤ê°€\n",
    "        })\n",
    "\n",
    "        # â˜… 1) ìµœì¢… ë¡œê·¸\n",
    "        print(f\"[ê²°ê³¼] {pid}: {ì£¼ë§íŒë§¤ê°€}\")\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "# === ì‹¤í–‰ ===\n",
    "df_result = ì•¼ë†€ì_ì£¼ë§íŒë§¤ê°€(driver, ids)\n",
    "df_result.to_csv(\"0814ì•¼ë†€ì_ì£¼ë§íŒë§¤ê°€.csv\", index=False)\n",
    "df_result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464eb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(\"í”„ë¡œì íŠ¸/0814ì•¼ë†€ì_ì£¼ë§íŒë§¤ê°€í¬ë¡¤ë§.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d221b",
   "metadata": {},
   "source": [
    "# ìˆ™ì†Œ ì´ë¯¸ì§€ ë‹¤ìš´ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_image = pd.read_csv('ì´ë¯¸ì§€URL_ìµœì‹ 3ê°œ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3172828",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¬ëŸ¼ëª… ë³€ê²½\n",
    "df_image.rename(columns={'LDGS_CD' : 'ìˆ™ì†ŒID', 'GSRM_IMAGE_URL' : 'ì´ë¯¸ì§€URL'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f59e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# ì €ì¥ í´ë”\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "df_test = df_image\n",
    "\n",
    "# ìˆ™ì†ŒIDë³„ ìˆœë²ˆ ì¹´ìš´í„°\n",
    "counters = {}\n",
    "\n",
    "# ì‹¤íŒ¨ ID ëª©ë¡\n",
    "failed = []\n",
    "\n",
    "total = len(df_test)\n",
    "for i, row in enumerate(df_test.itertuples(), 1):\n",
    "    sid = str(row.ìˆ™ì†ŒID)\n",
    "    url = str(row.ì´ë¯¸ì§€URL).strip()\n",
    "\n",
    "    # ì§„í–‰ìƒí™© ì¶œë ¥\n",
    "    print(f\"[{i}/{total}] {sid}\")\n",
    "\n",
    "    # URLì´ ë¹„ì–´ìˆê±°ë‚˜ NaNì´ë©´ ì‹¤íŒ¨ ëª©ë¡ì— ì¶”ê°€\n",
    "    if not url or url.lower() == 'nan':\n",
    "        failed.append((sid, url, 'empty_url'))\n",
    "        continue\n",
    "\n",
    "    counters[sid] = counters.get(sid, 0) + 1\n",
    "    idx = counters[sid]\n",
    "    save_path = os.path.join(save_dir, f\"{sid}_{idx:02d}.jpg\")\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        if res.status_code == 200 and 'image' in res.headers.get('Content-Type', ''):\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            print(f\" {os.path.basename(save_path)} ì €ì¥ ì™„ë£Œ\")\n",
    "        else:\n",
    "            failed.append((sid, url, f\"status:{res.status_code}\"))\n",
    "    except Exception as e:\n",
    "        failed.append((sid, url, f\"error:{type(e).__name__}\"))\n",
    "\n",
    "# ì‹¤íŒ¨ ëª©ë¡ CSV ì €ì¥ (ì´ë¯¸ì§€URL í¬í•¨)\n",
    "if failed:\n",
    "    pd.DataFrame(failed, columns=['ìˆ™ì†ŒID', 'ì´ë¯¸ì§€URL', 'ì‚¬ìœ ']).to_csv('ì•¼ë†€ììˆ™ì†Œì´ë¯¸ì§€_ì‹¤íŒ¨1.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸš¨ ì‹¤íŒ¨ ID {len(failed)}ê±´ ì €ì¥ ì™„ë£Œ: ì•¼ë†€ììˆ™ì†Œì´ë¯¸ì§€_ì‹¤íŒ¨1.csv\")\n",
    "else:\n",
    "    print(\"\\n ëª¨ë“  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# ì €ì¥ í´ë”\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# â‘  ì‹¤íŒ¨ ëª©ë¡ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_failed = pd.read_csv(\"ì•¼ë†€ììˆ™ì†Œì´ë¯¸ì§€_ì‹¤íŒ¨1.csv\", dtype={'ìˆ™ì†ŒID': str})\n",
    "\n",
    "# ìˆ™ì†ŒIDë³„ ìˆœë²ˆ ì¹´ìš´í„°\n",
    "counters = {}\n",
    "\n",
    "total = len(df_failed)\n",
    "for i, row in enumerate(df_failed.itertuples(), 1):\n",
    "    sid = str(row.ìˆ™ì†ŒID)\n",
    "    url = str(row.ì´ë¯¸ì§€URL).strip()\n",
    "\n",
    "    print(f\"[{i}/{total}] {sid}\")\n",
    "\n",
    "    if not url or url.lower() == 'nan':\n",
    "        continue\n",
    "\n",
    "    counters[sid] = counters.get(sid, 0) + 1\n",
    "    idx = counters[sid]\n",
    "    save_path = os.path.join(save_dir, f\"{sid}_{idx:02d}.jpg\")\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        if res.status_code == 200 and 'image' in res.headers.get('Content-Type', ''):\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            print(f\" {os.path.basename(save_path)} ì €ì¥ ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(f\" âŒ ì €ì¥ ì‹¤íŒ¨: status:{res.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\" âŒ ì €ì¥ ì‹¤íŒ¨: error:{type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•¼ë†€ìocr ì´ë¯¸ì§€ ê°¯ìˆ˜ í™•ì¸\n",
    "image_files = [f for f in os.listdir(save_dir) if f.endswith('.jpg')]\n",
    "print(f\"ì•¼ë†€ìocrì— ì €ì¥ëœ ì´ë¯¸ì§€ ìˆ˜: {len(image_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53187f27",
   "metadata": {},
   "source": [
    "- ì´ë¯¸ì§€ íŒŒì¼ ëˆ„ë½ê°’ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcba87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ì €ì¥ í´ë” ê²½ë¡œ\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "\n",
    "# ì €ì¥ëœ ì´ë¯¸ì§€ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸\n",
    "image_files = [f for f in os.listdir(save_dir) if f.lower().endswith('.jpg')]\n",
    "print(f\"ì•¼ë†€ìocrì— ì €ì¥ëœ ì´ë¯¸ì§€ ìˆ˜: {len(image_files)}\")\n",
    "print(f\"df_image í–‰ ìˆ˜: {len(df_image)}\")\n",
    "\n",
    "# ì €ì¥ëœ ìˆ™ì†ŒID ì¶”ì¶œ\n",
    "saved_ids = set([name.split('_')[0] for name in image_files])\n",
    "all_ids = set(df_image['ìˆ™ì†ŒID'].astype(str))\n",
    "\n",
    "# ëˆ„ë½ëœ ìˆ™ì†ŒID\n",
    "missing_ids = all_ids - saved_ids\n",
    "print(f\"ëˆ„ë½ëœ ìˆ™ì†ŒID ê°œìˆ˜: {len(missing_ids)}\")\n",
    "\n",
    "# ëˆ„ë½ëœ ìˆ™ì†ŒIDì™€ ì´ë¯¸ì§€URL ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "missing_df = df_image[df_image['ìˆ™ì†ŒID'].astype(str).isin(missing_ids)][['ìˆ™ì†ŒID', 'ì´ë¯¸ì§€URL']].reset_index(drop=True)\n",
    "missing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "image_files = [f for f in os.listdir(save_dir) if f.lower().endswith('.jpg')]\n",
    "\n",
    "# 1. df_imageì—ì„œ {ìˆ™ì†ŒID: [ì´ë¯¸ì§€URL, ...]} dict ìƒì„±\n",
    "id2urls = {}\n",
    "for _, row in df_image.iterrows():\n",
    "    sid = str(row['ìˆ™ì†ŒID'])\n",
    "    id2urls.setdefault(sid, []).append(row['ì´ë¯¸ì§€URL'])\n",
    "\n",
    "# 2. ì €ì¥ëœ íŒŒì¼ëª…ì—ì„œ ë°”ë¡œ URL ì¶”ì¶œ\n",
    "saved_urls = set()\n",
    "for fname in image_files:\n",
    "    sid, idx = fname.split('_')[0], int(fname.split('_')[1].split('.')[0])\n",
    "    urls = id2urls.get(sid, [])\n",
    "    if 0 < idx <= len(urls):\n",
    "        saved_urls.add(urls[idx-1])\n",
    "\n",
    "# 3. ëˆ„ë½ëœ ì´ë¯¸ì§€URL ì°¾ê¸°\n",
    "all_urls = set(df_image['ì´ë¯¸ì§€URL'])\n",
    "missing_urls = all_urls - saved_urls\n",
    "print(f\"ëˆ„ë½ëœ ì´ë¯¸ì§€URL ê°œìˆ˜: {len(missing_urls)}\")\n",
    "\n",
    "# 4. ëˆ„ë½ëœ ìˆ™ì†ŒIDì™€ ì´ë¯¸ì§€URL ë°ì´í„°í”„ë ˆì„\n",
    "missing_df = df_image[df_image['ì´ë¯¸ì§€URL'].isin(missing_urls)][['ìˆ™ì†ŒID', 'ì´ë¯¸ì§€URL']].reset_index(drop=True)\n",
    "missing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dada4f",
   "metadata": {},
   "source": [
    "- 154ê°œ ëˆ„ë½ê°’ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "image_files = [f for f in os.listdir(save_dir) if f.lower().endswith('.jpg')]\n",
    "\n",
    "# 1. íŒŒì¼ëª…ì—ì„œ ìˆ™ì†ŒIDë³„ë¡œ ì €ì¥ëœ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "from collections import defaultdict\n",
    "saved_idx_dict = defaultdict(set)\n",
    "for fname in image_files:\n",
    "    sid, idx = fname.split('_')[0], int(fname.split('_')[1].split('.')[0])\n",
    "    saved_idx_dict[sid].add(idx)\n",
    "\n",
    "# 2. df_failedì—ì„œ ê¸°ëŒ€í•˜ëŠ” ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "expected_idx_dict = defaultdict(list)\n",
    "for _, row in df_failed.iterrows():\n",
    "    sid = str(row['ìˆ™ì†ŒID'])\n",
    "    expected_idx_dict[sid].append(row['ì´ë¯¸ì§€URL'])\n",
    "\n",
    "# 3. ëˆ„ë½ëœ ë²ˆí˜¸ íŒŒì•… ë° ëˆ„ë½ ì´ë¯¸ì§€URL ì¶”ì¶œ\n",
    "missing_list = []\n",
    "for sid, urls in expected_idx_dict.items():\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        if i not in saved_idx_dict[sid]:\n",
    "            missing_list.append((sid, i, url))\n",
    "\n",
    "# 4. ëˆ„ë½ëœ ìˆ™ì†ŒID, ë²ˆí˜¸, ì´ë¯¸ì§€URL ë°ì´í„°í”„ë ˆì„\n",
    "missing_df = pd.DataFrame(missing_list, columns=['ìˆ™ì†ŒID', 'ë²ˆí˜¸', 'ì´ë¯¸ì§€URL'])\n",
    "print(f\"ëˆ„ë½ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(missing_df)}\")\n",
    "missing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc345e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_dfì—ì„œ 3014319í˜¸ì¶œ\n",
    "missing_3014319 = missing_df[missing_df['ìˆ™ì†ŒID'] == \"3014319\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9cf13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# missing_3014319ì˜ ê° í–‰ì— ëŒ€í•´ ì´ë¯¸ì§€ ì €ì¥\n",
    "for i, row in missing_3014319.iterrows():\n",
    "    sid = str(row['ìˆ™ì†ŒID'])\n",
    "    idx = int(row['ë²ˆí˜¸']) if 'ë²ˆí˜¸' in missing_3014319.columns else i+1\n",
    "    url = str(row['ì´ë¯¸ì§€URL']).strip()\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"{sid}_{idx:02d}.jpg\")\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        if res.status_code == 200 and 'image' in res.headers.get('Content-Type', ''):\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            print(f\"{os.path.basename(save_path)} ì €ì¥ ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: status:{res.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: error:{type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "image_files = [f for f in os.listdir(save_dir) if f.lower().endswith('.jpg')]\n",
    "\n",
    "# 3ê°œë¡œ ê· ë“± ë¶„í• \n",
    "n = len(image_files)\n",
    "split_size = math.ceil(n / 3)\n",
    "splits = [image_files[i*split_size:(i+1)*split_size] for i in range(3)]\n",
    "\n",
    "# íŒŒì¼ë¡œ ì €ì¥\n",
    "for idx, split in enumerate(splits, 1):\n",
    "    df_split = pd.DataFrame(split, columns=['íŒŒì¼ëª…'])\n",
    "    out_path = f\"ì•¼ë†€ìocr_split{idx}.csv\"\n",
    "    df_split.to_csv(out_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"{out_path} ì €ì¥ ì™„ë£Œ: {len(df_split)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "image_files = [f for f in os.listdir(save_dir) if f.lower().endswith('.jpg')]\n",
    "\n",
    "# 3ê°œë¡œ ê· ë“± ë¶„í• \n",
    "n = len(image_files)\n",
    "split_size = math.ceil(n / 3)\n",
    "splits = [image_files[i*split_size:(i+1)*split_size] for i in range(3)]\n",
    "\n",
    "# í´ë”ë¡œ ì´ë™\n",
    "for idx, split in enumerate(splits, 1):\n",
    "    split_dir = f\"ì•¼ë†€ìocr_part{idx}\"\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    for fname in split:\n",
    "        src = os.path.join(save_dir, fname)\n",
    "        dst = os.path.join(split_dir, fname)\n",
    "        shutil.move(src, dst)\n",
    "    print(f\"{split_dir}ë¡œ {len(split)}ê°œ ì´ë¯¸ì§€ ì´ë™ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_02.jpg'ë¡œ ëë‚˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ì €ì¥\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "image_files = [f for f in os.listdir(save_dir) if f.lower().endswith('_02.jpg')]\n",
    "\n",
    "# íŒŒì¼ëª…ë§Œ ì €ì¥\n",
    "df_02 = pd.DataFrame(image_files, columns=['íŒŒì¼ëª…'])\n",
    "df_02.to_csv(\"ì•¼ë†€ìocr_02only.csv\", index=False, encoding='utf-8-sig')\n",
    "print(f\"ì•¼ë†€ìocr_02only.csv ì €ì¥ ì™„ë£Œ: {len(df_02)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df37602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_02.jpg'ë¡œ ëë‚˜ëŠ” ì´ë¯¸ì§€ë§Œ ìƒˆë¡œìš´ í´ë”ë¡œ ë³µì‚¬\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "save_dir = \"ì•¼ë†€ìocr\"\n",
    "target_dir = \"ì•¼ë†€ìocr_02only\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "image_files = [f for f in os.listdir(save_dir) if f.lower().endswith('_02.jpg')]\n",
    "\n",
    "for fname in image_files:\n",
    "    src = os.path.join(save_dir, fname)\n",
    "    dst = os.path.join(target_dir, fname)\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "print(f\"{target_dir} í´ë”ë¡œ {len(image_files)}ê°œ ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
